# Getting Started with Apache Spark

> Apache Spark is an **in-memory cluster computing framework** which is <u>designed to handle a wide range of big data workloads</u>

**Application of Apache Spark**

1. Data Integration and ETL
2. Batch Computation
3. Stream Processing
4. Machine Learning Analytics
5. Graph Computation

**Important Points**

* Apache Spark is natively written in Scala
* 

## What is PySpark

> PySpark is a Python API for Apache Spark

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcXUIm-aO2OPY9613Ze2ER6QhUr8A7gKfzKN0DaVnDHdd1HYCRk7eCEJHo13z2Fepn95CDwBgL4sOX60pGz1sZQ0tIm-nlMySCX69DWr1K-zF3xwIPBB-pDnzdHc88rYf-gfmnwk91OGdAqxZIJnt5pp1Tr?key=_he-T4Jq934AhrSZa-Be-g)



## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXey4Ke38OFBZlyXSqnUR-8b9ce0fVaV8O2JKFBmGc8J-59KMCqYY2Pao1bBMuoxgZG8sYBB9H1LHYz8mWB6xb3thUdkdWu19AgHHm3F1_3AA7js3cDZvEz6AN0XoRUlChXA_R9prwrEzF3ArlYya0WMZbTO?key=_he-T4Jq934AhrSZa-Be-g)



## Spark Shell

1. Spark Shell
   * Scala language
2. PySpark Shell
   1. Python

## Building Spark Application

1. Load the data (Data Structure)
2. Process the data
3. Store the results

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXc8HCLr83oBfverZtuMJNW5qQZHYuWSSXxzW8gi06fqzitfrz9Px0kqYzgvCGhz6wDcGxZkAA-X76jlkmrta0T2meYZLH3826J5ECJq5mkT4y22Ym3NFmVdMeV9MzbRZ8vNb-hL9q-KkFHyvgVzEC58ppU?key=_he-T4Jq934AhrSZa-Be-g)



## RDD's (Resilient Distributed Dataset)

> RDD's are the fundamental data structure of Apache Spark

**Partitions**

An RDD is a collection of partitions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeXtT9sVBb_wYTri_J-VkxN3HoBf5t-GW5roDuoG7cpPfT1xjPiKxF00fVY1uss6cd3ZX0IFn8XV5XaoewsXMIV6xF4xNEcDs8AcTUL-KutfRRUOD9qLSIF504HCE4n2tirBh4aj9A7TED2JpdFMCOP2d_w?key=Dxp7lTxgvspH2ig-I7LuEw)



![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe6fjY9Tr9JldJoKQLM8_9PQd1z65zWIr0p7MLoRIkCs4WJD5HIKqXD_2bsRwuniPCRSn5FQqiNFXmktTYlRAA4qiQHdDmzoTm5-xzfZIqhN1Gj0jgVhOIKa6XajtzbqdNxx23zbMS2M4MMNgXRuyysSZQj?key=Dxp7lTxgvspH2ig-I7LuEw)



## RDD Creation

```
"abc".upper()
```

There are two main ways

1. Create an RDD from Collection

   ```python
   L = [1,2,3,4,5,6,7,8,9,10]
   rdd = sc.parallelize(L)
   ```

2. Create an RDD from external source

   ```python
   sales_data = "c:/sales.csv"
   rdd = sc.textFile(sales_data)
   ```

   

**Important**

* All the methods to create an RDD is present inside SparkContext (sc)



## RDD/DataFrame Operations

Two types of operations

1. Transformations
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf1vs1hhhgmbhcl3p4HNdKCwX-z6tzO4RZiqklrAseSKGSnUi7sef8XxfXfCiI5yL9vfNbSPPl6ae0YcfZvU3tVipjYqePqPWYChjSAduXle7HwHM_vbwRe6RH2tZ44YrvEaLjJxPNYpprfN1SV_roEgkTT?key=Dxp7lTxgvspH2ig-I7LuEw)



### Transformation

* Once an RDD is created we can apply transformation, on applying transformation we get another RDD (Immutable)

### Actions

* We can perform an action on an RDD; The final computation happens  only when you execute an action; the actual job will be triggered only when we perform an action
